import polars as pl

from onsides import stringsearch
from onsides.predict import predict
from onsides.types import IndexedText
from onsides.stringsearch import MatchContext

langs = ["english", "japanese"]


rule all:
    input:
        expand("_onsides/vocab/meddra_{language}.parquet", language=langs),
        expand("_onsides/combined/label_{language}_preds.parquet", language=["english"]),
        # expand("_onsides/combined/label_{language}_preds.parquet", language=langs)


rule build_vocabulary:
    input:
        queries = "snakemake/onsides/evaluate/meddra_vocabs.sql",
        mrconso = "data/mrconso.rrf"
    output:
        expand(
            "_onsides/vocab/meddra_{language}.parquet",
            language=langs
        )
    shell: "duckdb duck.db < {input.queries}"


rule build_labels:
    input:
        us = "_onsides/us/label_text.parquet",
        uk = "_onsides/uk/label_text.parquet",
        eu = "_onsides/eu/label_text.parquet",
        jp = "_onsides/jp/med_label_text.parquet",
        queries = "snakemake/onsides/evaluate/format_labels.sql",
    output:
        "_onsides/combined/english_labels.parquet",
        "_onsides/combined/japanese_labels.parquet",
    shell: "duckdb < {input.queries}"


rule string_match:
    input:
        labels = "_onsides/combined/{language}_labels.parquet",
        vocabulary = "_onsides/vocab/meddra_{language}.parquet",
    output: "_onsides/combined/label_{language}_string_match.parquet"
    run:
        # Load the labels themselves
        raw_labels = pl.read_parquet(input.labels).to_dicts()
        labels = [IndexedText.model_validate(x) for x in raw_labels]
        print(f"Found {len(labels)} labels")

        # Load the vocabularies (which we are trying to match)
        raw_terms = pl.read_parquet(input.vocabulary).to_dicts()
        terms = [IndexedText.model_validate(x) for x in raw_terms]
        print(f"Matching {len(terms)} terms")

        # Find exact string matches
        matches = stringsearch.parse_texts(texts=labels, terms=terms)
        print(f"Found {len(matches)} string matches")
        pl.DataFrame(matches).write_parquet(output[0])


rule evaluate_onsides:
    input: "_onsides/combined/label_{language}_string_match.parquet"
    output: "_onsides/combined/label_{language}_preds.parquet"
    resources: jobs=1
    run:
        raw_matches = pl.read_parquet(input[0]).to_dicts()
        matches = [MatchContext.model_validate(x) for x in raw_matches]
        print(f"Found {len(matches)} raw matches")

        # Apply the BERT model to evaluate the matches in their contexts
        match_texts = [m.context for m in matches]
        network_path = Path(
            "/data1/home/berkowitzj/onsides/models/microsoft/"
            "BiomedNLP-PubMedBERT-base-uncased-abstract"
        )
        model_path = Path(
            "/data1/home/berkowitzj/onsides/models/"
            "bestepoch-bydrug-PMB_14-ALL-125-all_222_24_25_1e-06_256_32.pth"
        )
        predictions = predict(
            texts=match_texts,
            network_path=network_path,
            weights_path=model_path,
            text_settings=None,
            batch_size=5_000,
        )
        (
            pl.concat([
                pl.DataFrame(matches),
                pl.DataFrame(predictions, schema=["pred0", "pred1"])
            ], how="horizontal")
            .write_parquet(output[0])
        )
